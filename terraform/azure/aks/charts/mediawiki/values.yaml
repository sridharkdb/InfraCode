# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
app:
  port: 8080
lamp:
  # wether to use unix sockets or tcp to communicate with php-fpm
  fcgi_mode: FCGI_UNIX
  # Allows to distinguish between installations
  servergroup: default
  ### HTTPD section ###
  httpd:
    # The version of the httpd-fcgi image
    version: latest
    # The port apache will be listening on
    port: 8080
    # Requests and limits for httpd
    requests:
      cpu: 500m
      memory: 200M
    limits:
      cpu: 500m
      memory: 200M
    # custom apache configuration to inject
    custom_config: ""
    # The version of the prometheus-httpd-exporter image
    exporter_version: latest
  phpfpm:
    # data about the image
    image:
      # The docker image for the main app
      name: __image_name__
      # the version of said image
      version: latest
      # WMF pipeline specific: we can have multiple "flavours" of the same release.
      # Leave blank to omit.
      flavour: ~
    # php-fpm configuration
    # Opcache tunables
    opcache:
      # Opcache size
      size: "100M"
      # maximum number of accelerated files.
      nofiles: 1024
    # time in seconds after which a slow request will be logged.
    slowlog_timeout: 1
    # timeout for a request
    timeout: 60
    # number of workers. Typically 2x the CPU cores.
    workers: 8
    # APCu configuration
    apc:
      size: 200M
    requests:
      cpu: 1
      memory: 500M
    limits:
      cpu: 2
      memory: 1G
    # Version of the phpfpm exporter
    exporter_version: latest
monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false
  # The ports to scrape are all labeled -metrics here.
  named_ports: true
service:
  deployment: minikube # valid values are "production" and "minikube"
  port:
    name: http # a unique name of lowercase alphanumeric characters or "-", starting and ending with alphanumeric, max length 63
    # protocol: TCP # TCP is the default protocol
    targetPort: 8080 # the number or name of the exposed port on the container
    port: 80 # the number of the port desired to be exposed to the cluster
    nodePort: null # you need to define this if "production" is used. In minikube environments let it autoallocate
config:
# Add here any secrets you might need
  private: {}

# Additional resources if we want to add a port for a debugger to connect to.
debug:
  enabled: false
  # Define here any port that you want to expose for debugging purposes
  ports: []

# Basic mesh-related data.
mesh:
  enabled: false
  admin: {port: 1666 }
  image_version: latest
  # http keepalive timeout for incoming requests
  idle_timeout: "4.5s"
  # Port to listen to
  public_port: 80
  local_access_log_min_code: "200"
  # Headers to add to a local request,
  # in dictionary form.
  request_headers_to_add: []
  # Timeout of a request to the local service
  upstream_timeout: "60s"
  # Enabling telemetry, telemetry port.
  telemetry:
    enabled: true
    port: 1667
  resources:
    requests:
      cpu: 200m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

# Mesh-related discovery
# TODO: move under mesh.* once we can
discovery:
  # List of listeners
  listeners: []

# Mesh related pure TCP proxies
tcp_proxy:
  listeners: []

# Should be provided by configuration management.
# See details of the structures in the comments
# In the configuration module.
services_proxy: ~
tcp_services_proxy: ~

# Allow external traffic to reach this service via a (cluster provided) ingress controller.
# https://wikitech.wikimedia.org/wiki/Kubernetes/Ingress#Configuration_(for_service_owners)
ingress:
  enabled: false
  # By default, enabling ingress will switch the charts services from type NodePort to
  # ClusterIP. While that is fine for new services it may not be desired during transition
  # of existing ones from dedicated LVS to Ingress.
  # By setting keepNodePort to true, the services will stay of type NodePort.
  keepNodePort: false
  # gatewayHosts settings configure the hostnames this service will be reachable on.
  # By default, this will be a list like:
  # - {{ gatewayHosts.default }}.{{ domain }}
  # For all domains listed in .gatewayHosts.domains (specified by SRE for each environment)
  gatewayHosts:
    # default will expand to {{ .Release.Namespace }} as long as it is an empty string.
    default: ""
    # disableDefaultHosts may be set to true if the service should not be reachable via
    # the gateway hosts generated by default (see above).
    disableDefaultHosts: false
    # extraFQDNs ist a list of extra FQDNs this service should be reachable on.
    # It can be used to extend the gateway hosts that are generated by default.
    extraFQDNs: []
  # If you want to attach routes of this release to an existing Gateway, provide the name
  # of that gateway here in the format: <namespace>/<gateway-name>
  # This is useful if you wish to make multiple releases available from the same hostname.
  existingGatewayName: ""
  # routeHosts is a list of FQDNs the httproutes should attach to.
  # If existingGatewayName not set, this list might be empty and will default to the gateway
  # host generated according to how .Values.gatewayHosts.* is configured.
  # If existingGatewayName is set, you need to provide the FQDNs your routes should attach to.
  routeHosts: []
  # HTTPRoute routing rules. By default https://<hosts from above>/ will be routed to
  # the service without modification.
  # Docs: https://istio.io/v1.9/docs/reference/config/networking/virtual-service/#HTTPRoute
  httproutes: []
  # Base CORS HTTP headers for the general use case.
  base_cors_policy: false
  # Add a custom CORS policy, injecting an Istio CorsPolicy config:
  # https://istio.io/latest/docs/reference/config/networking/virtual-service/#CorsPolicy
  # Takes precedence over base_cors_policy.
  custom_cors_policy: {}

common_images:
  statsd:
    image: prometheus-statsd-exporter
    version: latest
# WARNING: If you want to enable the module,
# you will need to add a "statsd" stanza to monitoring
# see modules/base/values.yaml for reference.

common_images:
  mcrouter:
    mcrouter: latest
    exporter: latest
cache:
  mcrouter:
    # This is the route prefix that will be added by default
    # to all requests whose key doesn't begin with /
    port: 11213
    route_prefix: /default
    cross_region_timeout: 250
    cross_cluster_timeout: 100
    num_proxies: 3
    probe_timeout: 6000
    timeouts_until_tko: 3
    zone: foo
    resources:
      requests:
        cpu: 250m
        memory: 200M
      limits:
        cpu: 250m
        memory: 200M
    pools:
      - name: foo-pool
        servers:
          - 192.168.1.1
          - 192.168.1.2
        failover:
          - 192.168.1.54
          - 192.168.1.53
        zone: foo
      - name: bar-pool
        servers:
          - 192.168.2.1
          - 192.168.2.2
        failover:
          - 192.168.2.54
          - 192.168.2.53
        routes:
          # Route 1: simple standalone
          - route: /default
            pool: foo-pool
            failover_time: 0
          # Route 2: "replica"
          # Actually generates two routes,
          # * /replica/foo that reads and writes to the foo pools
          # * /replica/bar that reads from foo and writes to bar
          # Applications can write to both using /replica/*/ as a
          # prefix.
          - route: /replica/foo
            pool: foo-pool
            failover_time: 10
            replica:
              route: /replica/bar
              pool: bar-pool
          # Route 3: "warmup"
          # This route will try reading a key from the local pool, and
          # if it doesn't find it, it will contact the bar pool and store
          # the resulting key for the ttl time
          - route: /multilayer
            pool: bar-pool
            failover_time: 10
            warmup:
              pool: foo-pool
              ttl: 60

docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent
resources:
  replicas: 1

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false

networkpolicy:
  egress:
    enabled: false

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# Cronjob definitions
# Here you can define your cronjobs
cronjobs: []
# Example of a job:
# - name: my-cron-hourly
#   enabled: true
#   command:
#      - /bin/cowsay
#      - "hello"
#   schedule: "@hourly" (defaults to @daily)
#   concurrency: Replace (defaults to "Forbid")
#   image_versioned: my-app:1.1.1 (defaults to the app used in the main application definition)
#   resources: (optional list of requests/limits for our cronjob; if not present will default to the application ones.)

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}